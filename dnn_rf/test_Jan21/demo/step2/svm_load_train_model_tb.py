# Load a dnn model from .pb file
# Train and test model using data from file training_set

# Input: 
# 1. files generated by make_dnn.py 
# 2. training_set (may need to modify the inputfile path)

from numpy import array
from numpy import argmax
import tensorflow as tf
import numpy as np
import os
import matplotlib.pyplot as plt
import sklearn.metrics as metrics
import time
from tensorflow.python.platform import gfile
from sklearn.datasets import load_svmlight_file
from sklearn import preprocessing
from tensorflow.contrib.tensor_forest.client import *
from tensorflow.contrib.tensor_forest.python import *
from tensorflow.python.ops import resources


classifier = "dnn" # "rf" # dnn
graph_pb = classifier + '/graph/model'

graph_def = tf.GraphDef()
export_dir = classifier + "/trained/model"

graph_pb = 'dnn_graph/model'

graph_def = tf.GraphDef()
export_dir = "dnn_trained/model"
# builder = tf.saved_model.builder.SavedModelBuilder(export_dir)


# with open(graph_pb, 'rb') as f:
# 	graph_def.ParseFromString(f.read())
# tf.import_graph_def(graph_def, name='')

sess = tf.Session()
graph = tf.saved_model.loader.load(sess, ['tag'],graph_pb)


accuracy       = sess.graph.get_tensor_by_name('accuracy:0')
nbr_features   = sess.graph.get_tensor_by_name('nbr_features:0')
input_features = sess.graph.get_tensor_by_name('x:0')
input_labels   = sess.graph.get_tensor_by_name('y_:0')
keep_prob      = sess.graph.get_tensor_by_name('keep_prob:0')
loss_optimizer = sess.graph.get_tensor_by_name('loss_optimizer:0')
# correct_prediction = sess.graph.get_tensor_by_name('correct_prediction:0')
init_op        = sess.graph.get_operation_by_name('init')
try:
	init_vars        = sess.graph.get_operation_by_name('init_vars')
except KeyError:
	pass

softmaxed_logits = sess.graph.get_tensor_by_name('softmaxed_logits:0')
# tensorboard
tf.summary.scalar('accuracy', accuracy)
merged = tf.summary.merge_all()
train_writer = tf.summary.FileWriter('./TFlogs_'+classifier, sess.graph)
try:
	sess.run(init_vars)
except NameError:
	pass
sess.run(init_op)

# first get the limits 
col_size = sess.run(nbr_features)
batch_size = 40
training_data = load_svmlight_file("../../../ps_data/train-0.svm")
Xr = training_data[0].todense()[:,0:col_size]
n = training_data[0].shape[0]
yr = training_data[1]
lb = preprocessing.LabelBinarizer()
yr = lb.fit_transform(training_data[1])
if input_labels.get_shape().as_list()[1] == 2:
	yr = np.column_stack([yr, 1-yr])

print ('Start training: ')
for i in range(20):

	for j in range(int(n/batch_size)):
		batch_xs = Xr[j*batch_size: (j+1)*batch_size-1]
		batch_ys = yr[j*batch_size: (j+1)*batch_size-1]	
		sess.run(loss_optimizer, feed_dict={input_features: batch_xs, input_labels: batch_ys, keep_prob: 0.5})

	if i % 2 is 0:     
		print ('epoch: ' + str(i))
		summary,acc = sess.run([merged, accuracy], feed_dict={input_features: Xr, input_labels: yr, keep_prob: 1.0})
		train_writer.add_summary(summary,i)
		print (acc)

builder = tf.saved_model.builder.SavedModelBuilder(export_dir)
builder.add_meta_graph_and_variables(sess, ['tag'])
builder.save()
sess.close()


