# Load a dnn model from myM.meta file
# Train and test model using data from file training_set

# Input: 
# 1. files generated by ps_save_graph.py 
# 2. training_set (may need to modify the inputfile path)

from numpy import array
from numpy import argmax
import tensorflow as tf
import numpy as np
import os
import matplotlib.pyplot as plt
import sklearn.metrics as metrics
import time
from tensorflow.python.platform import gfile
from sklearn.datasets import load_svmlight_file
from sklearn import preprocessing

from tensorflow.contrib.tensor_forest.client import *
from tensorflow.contrib.tensor_forest.python import *
from tensorflow.python.ops import resources


class Data:
	def __init__(self, X, labels, labels_sca):
		self.X = X
		self.labels = labels
		self.labels_sca = labels_sca

def get_data():
    	# data = load_svmlight_file("./train-0.svm")
    	data = load_svmlight_file("../../ps_data/train-0.svm")
    	return data[0], data[1]

def main():

	st_time = time.time()
	train_percentage = 0.67
	# training_data = load_svmlight_file("./train-0.svm")
	training_data = load_svmlight_file("../../ps_data/train-0.svm")
	Xr = training_data[0].todense()
	lb = preprocessing.LabelBinarizer()
	yr = lb.fit_transform(training_data[1])
	testing_data = training_data # load_svmlight_file("./testing.svm") 
	n = training_data[0].shape[0]

	graph_pb = 'dnn_only_v2.pb'
	graph_def = tf.GraphDef()

	with open(graph_pb, 'rb') as f:
		graph_def.ParseFromString(f.read())
	tf.import_graph_def(graph_def, name='')

	# sess = tf.Session()
	sess = tf.InteractiveSession()
	
	accuracy = sess.graph.get_tensor_by_name('accuracy:0')
	nbr_features_graph = sess.graph.get_tensor_by_name('nbr_features:0')
	x = sess.graph.get_tensor_by_name('x:0')
	y_ = sess.graph.get_tensor_by_name('y_:0')
	keep_prob = sess.graph.get_tensor_by_name('keep_prob:0')
	loss_optimizer = sess.graph.get_tensor_by_name('loss_optimizer:0')
	init_op = sess.graph.get_operation_by_name('init')
	cross_entropy = sess.graph.get_tensor_by_name('cross_entropy:0')

	# 
	tf.summary.scalar('accuracy', accuracy)
	tf.summary.scalar('cross_entropy', cross_entropy)
	merged = tf.summary.merge_all()
	train_writer = tf.summary.FileWriter('./logs', sess.graph)
	# b = tf.Variable(2.0, name = 'b')
	sess.run(init_op)
	# sess.run(tf.global_variables_initializer())

	# saver = tf.train.Saver()
	print(sess.run(nbr_features_graph))

	
	batch_size = 40
	
	if y_.get_shape().as_list()[1] == 2:
		yr = np.column_stack([yr, 1-yr])
	

	# print(sess.run(loss_optimizer, feed_dict={x: Xr, y_:yr, keep_prob: 0.5}))
	for i in range(2):
		
		# train the whole epoch (first shuffle the data)
		idx = np.arange(0, n)
		#np.random.shuffle(idx)
		#X_shuffle = [Xr[k] for k in idx]
		#labels_shuffle = [yr[k] for k in idx]

		for j in range(int(n/batch_size)):
			batch_xs = Xr[j*batch_size: (j+1)*batch_size-1]
			batch_ys = yr[j*batch_size: (j+1)*batch_size-1]	
			sess.run(loss_optimizer, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.5})

		if i % 2 is 0:     
			print ('epoch: ' + str(i))
			summary,acc, ce = sess.run([merged, accuracy, cross_entropy], feed_dict={x: Xr, y_: yr, keep_prob: 1.0})
			train_writer.add_summary(summary,i)
			print (acc)
			print (ce)

	
	# save_path = saver.save(sess, "dnn/dnn_model.ckpt")		
	# calculate acc using test data
	#acc_test, soft_logits_test = sess.run([accuracy, softmaxed_logits], feed_dict={x: testing_data[0].todense(), y_: testing_data[1], keep_prob: 1.0})
	#sk_auc_test = metrics.roc_auc_score(y_true = np.array(data_test.labels), y_score = np.array(soft_logits_test))
	
	#print ('test accuracy: ' + str(acc_test))
	#print('test sk auc: ' + str(sk_auc_test))
	
	sess.close()

	end_time = time.time()
	print('run time: '+ str(round(end_time-st_time)) + ' seconds')
	return 1

if __name__ == "__main__":
	result = main()

